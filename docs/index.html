<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>AP-COMP 221: Critical Thinking in Data Science</title>

  <!-- Font Awesome Icons -->
  <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Merriweather+Sans:400,700" rel="stylesheet">
  <link href='https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic' rel='stylesheet' type='text/css'>

  <!-- Plugin CSS -->
  <link href="vendor/magnific-popup/magnific-popup.css" rel="stylesheet">

  <!-- Theme CSS - Includes Bootstrap -->
  <link href="css/creative.min.css" rel="stylesheet">

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top py-3" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">AP-COMP 221: Critical Thinking in Data Science</a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto my-2 my-lg-0">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#intro">Introduction</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#related">Background</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#methods1">Methods</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#results1">Results</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#discussion">Discussion</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#references">References</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Masthead -->
  <header class="masthead">
    <div class="container h-100">
      <div class="row h-100 align-items-center justify-content-center text-center">
        <div class="col-lg-10 align-self-end">
          <h1 class="text-uppercase text-white font-weight-bold">Disparate Impacts and Sensitive Attributes in Automated Credit Scoring</h1>
          <hr class="divider my-4">
        </div>
        <div class="col-lg-8 align-self-baseline">
          <p class="text-white-75 font-weight-light mb-5">Assessing algorithmic bias of a popular credit default dataset
            <br>
            by Miruna G. Cristus, Everett Sussman, & Jan Geffert
            <br>
            <br>
            Find our Github repository <a href="https://github.com/JanGeffert/AC221-FinalProject">here</a>.
          </p>
          <a class="btn btn-primary btn-xl js-scroll-trigger" href="#abstract">Start Reading</a>
        </div>
      </div>
    </div>
  </header>




  <!-- About Section -->
  <section class="page-section bg-primary" id="abstract">
    <div class="container">
      <div class="row justify-content-center">
        <h2 class="text-white mt-0">Abstract</h2>
        <div class="col-lg-8 text-align-left">
          <hr class="divider light my-4">
          <p class="text-white mb-4" style="line-height:2.3em;">
            We investigate how credit-scoring systems might disproportionately impact vulnerable groups in receiving loans and credit lines by assessing their use of different sensitive demographic attributes (age, gender, marital status, education). Using a dataset of credit default behavior of Taiwanese credit card holders, we implemented three machine learning methods commonly used in literature for credit default studies (logistic regression, random forest, kNN), trained once on the set of features, and then on the set of features with the sensitive attributes removed. We find that there are negligible difference in accuracy for the models with and without sensitive attributes, raising questions of why these sensitive attributes were collected in the first place. We further find that our classifier exhibits disparate impact bias, erroneously flagging people with a high school education as likely to default more often than people with higher degrees.
          </p>
        </div>
        <!-- <div class="col-lg-8 text-center">
          <a class="btn btn-light btn-xl js-scroll-trigger" href="#thedata">Learn about the data</a>
        </div> -->
      </div>
    </div>
  </section>

  <!-- Services Section -->
  <section class="page-section" id="intro">
    <div class="container">
      <div class="row justify-content-center">
        <h2 class="text-black-center mt-0">Introduction</h2>
        <div class="col-lg-8 text-align-left">
          <hr class="divider dark my-4">
          <p class="text-black-70 mb-4" style="line-height:2.3em;">
            Much of the recent success of machine learning in a number of domains has been attributed to the emergence of large datasets, giving rise to adages such as “the more data, the better.” Accordingly, providers of prediction services, notably credit-scoring companies, have broadened the range of information they use to construct their models. At the same time, algorithmic decision-making systems in high-stakes scenarios have attracted increased scrutiny, as civil society and academia have raised concerns about algorithmic bias.
<br>
<br>
In this paper, we investigate how credit-scoring systems that make use of a number of sensitive demographic attributes might disproportionately impact vulnerable groups in receiving loans and credit lines, by analyzing dataset of default behavior of Taiwanese credit-card holders.
<br>
<br>
In the United States, the Equal Credit Opportunity Act (ECOA) enacted by Congress in 1974 prohibits credit discrimination based certain protected categories, such as race, age, or marital status [1,2]. While this information may be collected by creditors, it cannot be used in order to make decisions about whether to offer credit, or to determine the terms of the credit. ECOA applies to decisions made by algorithms just as much as to those made by people.
<br>
<br>
Yet, as economic status is known to be heavily correlated with protected attributes, such as race, ECOA expectedly does not remove all differences in treatment between subpopulations when it comes to loan refusals, credit card limits, etc. Women in the United States, for example, tend to have lower credit limits than men, due to the gender gap in annual income. Consequently, they tend to spend a higher percentage of their credit limit, which in turn negatively impacts their credit score. [3] This phenomenon could therefore easily lead to runaway feedback loops that entrench existing inequalities. It is an instance of disparate impact, where a "formally neutral" system adversely affects certain groups of people without its designers having explicitly intended such outcomes.
<br>
<br>
In order to understand the role of protected attributes in credit scoring and to assess the potential of disparate impact of credit-scoring algorithms, we examine a credit-default dataset published by Yeh et al. in 2009 [4], which features both sensitive demographic attributes (gender, marital status, age, education) and payment behavior information, as well as the target variable of whether a person has defaulted on their credit or not. We assess the predictive performance of three commonly used classification models: logistic regression, random forest, and k-nearest neighbors which we train on (A) the set of all features and (B) on the set of features excluding sensitive attributes. In a second step, we determine whether the prediction quality of the best-performing classifier differs for different subpopulations who share a sensitive attribute. Finally, we discuss our results in the context of more general considerations on computational bias and data collection practices.

          </p>
        </div>
      </div>
    </div>
  </section>


  <!-- Services Section -->
  <section class="page-section" id="related">
    <div class="container">
      <div class="row justify-content-center">
        <h2 class="text-black mt-0">Background</h2>
        <div class="col-lg-8 text-align-left">
          <hr class="divider dark my-4">
          <p class="text-black-70 mb-4" style="line-height:2.3em;">
            Credit scores have become an increasingly important part of people’s economic life, as they are used as a basis for many activities and services, from buying a house and leasing a car, to insurance rates or employment [5, 6]. If a person is predicted to be at high risk for defaulting and therefore does not get access to the credit they need, they might not qualify for a home loan, have difficulty in paying unplanned expenses (for example, an urgent medical issue), and in general have their financial stability threatened. As automatic prediction systems increasingly mediate such credit-scoring decisions, it is crucial to understand and mitigate the risk of reinscribing and calcifying historical discriminatory practices.
<br>
<br>
            Previous research is divided on whether credit scoring has a disparate impact on marginalized groups or not. A 2012 analysis of the National Fair Housing Alliance claims that not only do credit scores not accurately predict default rates but also that they disproportionately impact communities of color [7]. As mentioned above, other studies show a disparate impact of credit scoring on women compared to men [3]. However, another paper claims no evidence of disparate impact by race or gender, but limited disparate impact by age, where older individuals are at a disadvantage [8].
<br>
<br>
            The dataset which we analyze in this paper was selected due to its popularity: Hosted on the UCI machine repository, almost 350,000 web hits were recorded, 182 notebooks have been shared by data science practitioners on Kaggle, and 266 scientific papers cite the initial paper, according to Google Scholar. When reviewing this literature, we were shocked to find that little to none of the published research critically engages with the potential for discriminatory predictions. Gender and marital status, in fact, are claimed to be strong predictors in several of the proposed models [4] without any critical thought concerning their use. We further found no papers that explicitly address disparate impact or even the question of whether the inclusion of sensitive attributes was beneficial for predictive performance.
<br>
<br>
            We thus plan to contribute to the literature not only by assessing disparate impact on common algorithms in default predictions but also by analyzing whether accuracy is truly lost when algorithms do not use protected attributes. The latter question is important since governance debates about the use of personal information often center around a supposed trade-off between accuracy and utility of the system and privacy of the data subjects.
          </p>
        </div>
      </div>
    </div>
  </section>




  <!-- Services Section -->
  <section class="page-section" id="methods1">
    <div class="container">
      <div class="row justify-content-center">
        <h2 class="text-black mt-0">Methods: 1. Describing the Data</h2>
        <div class="col-lg-8 text-align-left">
          <hr class="divider dark my-4">
          <p class="text-black-70 mb-4" style="line-height:2.3em;">
            The dataset comprises <strong>30,000 observations</strong>, each describing a unique credit-card account holder in Taiwan in September, 2005. The explanatory variables include sensitive attributes (gender, age, marital status, education), as well as the account holder’s payment behavior for the preceding six months (repayment status, amount of bill statement, amount of previous payment). A binary variable denotes whether the credit card owner defaulted (1) on their loan or not (0). Within the sample, the <strong>rate of default payment</strong> is <strong>22.12%</strong>.
            <br>
            <br>

            <table class="table">
              <tbody>
                <tr>
                  <td>Credit limit</td>
                  <td>amount of credit offered to the credit card owner.</td>
                </tr>
                <tr>
                  <td>Gender</td>
                  <td>gender of the credit card holder; categories: male (1) or female (2).</td>
                </tr>
                <tr>
                  <td>Education</td>
                  <td>educational attainment of the credit card holder; categories: graduate school (1), university (2), high school (3), others (0, 4, 5, 6).</td>
                </tr>
                <tr>
                  <td>Marital status</td>
                  <td>marital status of the credit card holder; categories: married (1), single (2), divorced (3), others (0).</td>
                </tr>
                <tr>
                  <td>Age</td>
                  <td>age of the credit card holder.</td>
                </tr>
                <tr>
                  <td>History of payment</td>
                  <td>payment records for the previous 6 months, from April to September; categories: no consumption (-2), paid in full (-1), use of revolving credit (0), payment delay for one month (1), payment delay for 2 months (2), etc.</td>
                </tr>
                <tr>
                  <td>Amount of bill statement</td>
                  <td>amount of bill statements for the previous six months.</td>
                </tr>
                <tr>
                  <td>Amount of previous payment</td>
                  <td>amount of payment for bills in the previous six months.</td>
                </tr>
                <tr>
                  <td>Default</td>
                  <td><strong>1</strong> if the person defaulted on their payment, <strong>0</strong> otherwise.</td>
                </tr>
              </tbody>
            </table>
          </p>
        </div>
      </div>
    </div>
  </section>


  <section class="page-section" id="methods2">
    <div class="container">
      <div class="row justify-content-center">
        <h2 class="text-black mt-0">Methods: 2. Exploratory Data Analysis</h2>
        <div class="col-lg-8">
          <hr class="divider dark my-4">
          <p class="text-black-70 mb-4" style="line-height:2.3em;">
            A common observation in the field of algorithmic fairness is that the data used to train algorithmic decision-making systems already encodes outcomes of various historical discriminatory practices. Therefore a simple question worth asking is who is missing in this dataset. To this end, we compared the distributions of several demographic attributes in the dataset to that of the Taiwanese population overall.
             <br>
             <br>
             <br>
          </p>
        </div>


        <div class="col-lg-5">
          <h5 class="text-align-center">Age distribution of Taiwanese population</h5>
          <img class="img-fluid" src="img/population_age.png" alt="">
        </div>
        <div class="col-lg-5">
          <h5 class="text-align-center">Age distribution of the credit data</h5>
          <img class="img-fluid" src="img/credit_age.png" alt="">
        </div>
        <div class="col-lg-8">
          <p class="text-black-70 mb-4" style="line-height:2.3em;">
            <br>
            <br>
            The people who are found in the credit card data are adults who tend to be younger than the general Taiwanese population. Especially elderly citizens are not represented. When faced with a new, unseen credit-card holder who is 70 or older, an automated decision-making system, such as one powered by logistic regression, may therefore likely extrapolate rather than interpolate from seen data.
            <br>
            <br>
            <br>
          </p>
        </div>

        <div class="col-lg-5">
          <h5 class="text-align-center">Gender presentation of Taiwanese population</h5>
          <img class="img-fluid" src="img/population_gender.png" alt="">
        </div>
        <div class="col-lg-5">
          <h5 class="text-align-center">Gender presentation of the credit data</h5>
          <img class="img-fluid" src="img/credit_gender.png" alt="">
        </div>
        <div class="col-lg-8">
          <p class="text-black-70 mb-4" style="line-height:2.3em;">
            <br>
            <br>
            Women are overrepresented in the dataset. Further, the dataset’s M/F ontology reinscribes problematic binarized notions of gender, non-binary people are erased. [9]
            <br>
            <br>
            <br>
          </p>
        </div>


        <div class="col-lg-5">
          <h5 class="text-align-center">Marital Status of Taiwanese population</h5>
          <img class="img-fluid" src="img/population_marital.png" alt="">
        </div>
        <div class="col-lg-5">
          <h5 class="text-align-center">Marital Status of the credit data</h5>
          <img class="img-fluid" src="img/credit_marital.png" alt="">
        </div>
        <div class="col-lg-8">
          <p class="text-black-70 mb-4" style="line-height:2.3em;">
            <br>
            <br>
            While the majority of the Taiwanese population is married, the majority of the people represented in the credit dataset are single. The data further includes a disproportionately small number of divorced individuals. Widowed citizens are not represented explicitly in the credit data ontology.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section class="page-section" id="methods3">
    <div class="container">
      <div class="row justify-content-center">
        <h2 class="text-black mt-0">Methods: 3. Transformations</h2>
        <div class="col-lg-8">
          <hr class="divider dark my-4">
          <p class="text-black-70 mb-4" style="line-height:2.3em;">
            Inspecting the data, we also notice that some values of certain variables were undocumented (education: 0, 4, 5, 6; marital status: 0) prompting us to remove all observations including such values, resulting in a final count of <strong>29480</strong> observations (98.26% of original data). The removal of these observations does not meaningfully change the marginal distributions of the dataset’s features. Furthermore, we transform the “history of payment” variable, which in its original form encoded both ordinal and categorical information, for the purposes of model interpretability. Investigating options with both a categorical variable for the “no_delay” value in payment subgroups and a continuous variable to measure the delay payments, we concluded that in terms of both model interpretability and performance, it was best to lump the no-delay categories into only one (0). Thus, our new history of payment attribute is described by:
            <br>
            <br>
            <strong>History of payment:</strong> payment records for the previous 6 months, from April to September; categories: no delay (0), payment delay for one month (1), payment delay for 2 months (2), etc.
            <br>
            <br>
            Finally, we convert the categorical variables gender, education, and marital status into one-hot encodings.
            <br>
            <br>
          </p>
        </div>
      </div>
    </div>
  </section>


  <section class="page-section" id="methods4">
    <div class="container">
      <div class="row justify-content-center">
        <h2 class="text-black mt-0" style="width:400px;">Methods: 4. Modeling</h2>
        <div class="col-lg-8">
          <hr class="divider dark my-4">
          <p class="text-black-70 mb-4" style="line-height:2.3em;">
            In order to examine how computational bias may be present in the models proposed in literature, we train and evaluate classifiers on two sets of features: (A) a feature set comprising all features, and (B) a feature set that excludes sensitive attributes. By comparing the predictive performance of models trained on (A) with that of models trained on (B), we can determine whether any accuracy is lost via the exclusion of sensitive variables. We split the dataset uniformly at random into training and validation sets with a 80/20 ratio, retaining the label proportions.
            <br>
            <br>
            We train three models, tuning hyperparameters separately for each of the feature sets:
            <br>
            <ul>
              <li><strong>Logistic regression</strong> as a baseline and an interpretable parametric model</li>
              <li><strong>K-nearest neighbors</strong> as a nonparametric model</li>
              <li><strong>Random forest</strong> as a parametric model that we expected to reach high predictive power in practice with little parameter tuning</li>
            </ul>
            <br>
            In a second step, to examine disparate impact, we choose the best-performing classifier on feature set (B) and inspect its ROC curves for the following subpopulations:
            <br>
            <ul>
              <li><strong>Gender</strong> (m, f)</li>
              <li><strong>Age</strong> (younger than 35, 35-50, older than 50)</li>
              <li><strong>Marital status</strong> (single, married, divorced)</li>
              <li><strong>Educational attainment</strong> (highschool, university, graduate school)</li>
            </ul>
          </p>
        </div>
      </div>
    </div>
  </section>


  <section class="page-section" id="results1">
    <div class="container">
      <div class="row justify-content-center">
        <h2 class="text-black mt-0">Results: 1. Model Accuracy and Interpretation</h2>
        <div class="col-lg-8 text-align-left">
          <hr class="divider dark my-4">
          <p class="text-black-70 mb-4" style="line-height:2.3em;">
            The performance of the fitted models in terms of AUC, precision, recall and F1 scores are summarized in the table below. F1 score, rather than accuracy, was chosen as an additional figure of merit due to the aforementioned unbalanced label distribution.
            <br>
            <br>
          </p>
        </div>
        <div class="col-lg-10 text-align-left">
          <h4>Model Performance</h4>
          <table class="table">
            <thead>
              <tr>
                <td><strong>Model Type</strong></td>
                <td><strong>AUC </strong>(A)</td>
                <td><strong>AUC </strong> (B)</td>
                <td><strong>F1-Score </strong>(A)</td>
                <td><strong>F1-Score </strong>(B)</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Logistic Regression</td>
                <td>0.7624</td>
                <td>0.7478</td>
                <td>0.78</td>
                <td>0.78</td>
              </tr>
              <tr>
                <td>kNN</td>
                <td>0.6151</td>
                <td>0.6190</td>
                <td>0.72</td>
                <td>0.72</td>
              </tr>
              <tr>
                <td>Random Forest</td>
                <td><strong>0.7635</strong></td>
                <td><strong>0.7570</strong></td>
                <td><strong>0.79</strong></td>
                <td><strong>0.79</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="col-lg-8 text-align-left">

          <p class="text-black-70 mb-4" style="line-height:2.3em;">
          <br>
          For both feature sets, the best performing model in terms of the F1 score is random forest, matching our expectations and kNN is the worst performing model. Interestingly, there is no significant difference in overall performance between random forest and logistic regression; in fact, logistic regression in both cases has a higher F1 score for the “default” category than for the “no-default” category.
          </p>


        </div>
        <div class="col-lg-10 text-align-left">
          <h4>Logistic Regression Coefficients</h4>
          <table class="table">
            <thead>
              <tr>
                <td><strong>Feature</strong></td>
                <td><strong>Coefficient </strong> (A)</td>
                <td><strong>Coefficient </strong> (B) w/o sensitive attributes</td>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>age</td>
                <td>0.0000</td>
                <td>-</td>
              </tr>
              <tr>
                <td>gender_male</td>
                <td>0.1058</td>
                <td>-</td>
              </tr>
              <tr>
                <td>education_grad</td>
                <td>0.0000</td>
                <td>-</td>
              </tr>
              <tr>
                <td>education_univ</td>
                <td>0.0000</td>
                <td>-</td>
              </tr>
              <tr>
                <td>marriage_married</td>
                <td>0.1362</td>
                <td>-</td>
              </tr>
              <tr>
                <td>marriage_divorced</td>
                <td>0.0000</td>
                <td>-</td>
              </tr>
              <tr>
                <td>delay_0</td>
                <td>0.8901</td>
                <td>0.5892</td>
              </tr>
              <tr>
                <td>delay_2</td>
                <td>0.0833</td>
                <td>0.0359</td>
              </tr>
              <tr>
                <td>delay_3</td>
                <td>0.1542</td>
                <td>0.0280</td>
              </tr>
              <tr>
                <td>delay_4</td>
                <td>0.1025</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>delay_5</td>
                <td>0.1122</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>delay_6</td>
                <td>0.1214</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>all others<br>(limit_bal, bill_amount_0, ..., pay_amount_6)</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>

              <!-- <tr>
                <td>limit_bal</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>bill_amt_1</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>bill_amt_2</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>bill_amt_3</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>bill_amt_4</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>bill_amt_5</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>bill_amt_6</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>pay_amt_1</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>pay_amt_2</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>pay_amt_3</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>pay_amt_4</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>pay_amt_5</td>
                <td>0.0000</td>
                <td>0.0000</td>
              </tr>
              <tr>
                <td>pay_amt_6</td>
                <td>0.0000</td>
                <td>0.0000</td> -->
              </tr>
            </tbody>
          </table>
        </div>
        <div class="col-lg-8 text-align-left">

          <p class="text-black-70 mb-4" style="line-height:2.3em;">
            Interpreting the coefficients of the logistic regression on feature set A, the most important features are, in order of importance, the payment delay for the most recent month, third month, being married, being male, and lastly the payment delays for the other months (all positive). All other attributes have coefficients of 0, due to L1 regularization which we found to outperform L2 regularization. Intuitively, payment delays in recent months are more predictive of a person defaulting than in months further in the past. The model does make use of sensitive attributes, where being male (gender) and being married (marital status) correlate with an individual being more likely to default on their credit. Deploying such a model would be illegal according to ECOA.
<br>
<br>
            In a similar analysis of the feature importance statistics in random forest we notice that the payment delay for the most recent month has the highest feature importance, but is followed by the bill amount for the most recent month, and only then by the age of the credit owner, the credit limit, etc. Different from logistic regression, being married or being male have some of the lowest feature importance scores. This may be explained by the ability of random forest to model non-linearities in the data.
<br>
<br>
            Excluding sensitive attributes in feature set B, the only variables with non-zero coefficients in logistic regression are the payment delays for the previous 3 months (again, in decreasing order of coefficient magnitude as we go backwards in time). For random forest without sensitive attributes, the variables with the highest coefficients are the bill amount for the most recent month, the payment delay for the most recent month, the credit limit etc. There are only minor differences in terms of these scores and their order compared to the random forest with sensitive attributes.
          </p>

          <h5 class="text-align-center">Receiver Operating Characteristic Curve</h5>
          <img class="img-fluid" src="img/roc_plot.png" alt="">

          <p class="mb-4" style="line-height:2.3em;">
            Based on the results summarized above, we note that there is a very small difference in terms of performance between each method on the featuresets with sensitive attributes (A) and without sensitive attributes (B). The performance of the logistic regression and of the random forest in terms of the AUC is not significantly worse, while for kNN, the AUC actually increases when we exclude the sensitive variables. Our results suggest that, in this particular case, removing sensitive attributes would result in negligible performance degradation.
<br>
          </p>

        </div>
      </div>
    </div>
  </section>

  <section class="page-section" id="results2">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-8 text-align-left">
          <h2 class="text-black mt-0">Results 2: Disparate Impact</h2>
          <p class="mb-4" style="line-height:2.3em;">
            We select the random forest model trained on feature set B (the best model) in order to analyze its potential for disparate impact.
            <br>
            <h5 class="text-align-center">Gender, Age, Marital Status</h5>
          </p>
        </div>
      </div>
      <div class="row justify-content-center">
        <div class="col-lg-4 text-align-left">
          <img class="img-fluid" src="img/roc_by_gender_plot.png" alt="">
        </div>

        <div class="col-lg-4 text-align-left">
          <img class="img-fluid" src="img/roc_by_age_plot.png" alt="">
        </div>

        <div class="col-lg-4 text-align-left">
          <img class="img-fluid" src="img/roc_by_marital_plot.png" alt="">
        </div>
      </div>
      <div class="row justify-content-center">
        <p class="mb-4" style="line-height:2.3em;">
          The classifier does not perform significantly differently for subpopulations along the lines of gender, marital status, and age. (“divorced” for marital status had only few samples (<100), resulting in the jagged ROC curve). We do not notice any major discrepancies in false positive or true positive rates based on the sensitive categories.
        </p>
      </div>
      <div class="row justify-content-center">
        <p class="mb-4" style="line-height:2.3em;">
          <h5 class="text-align-center">Education</h5>
        </p>
      </div>
      <div class="row justify-content-center">
        <div class="col-lg-4 text-align-left">
          <img class="img-fluid" src="img/roc_by_edu_plot.png" alt="">
        </div>
      </div>
      <div class="row justify-content-center">
        <p class="mb-4" style="line-height:2.3em;">
            The classifier underperforms when making predictions for people with high school degrees as their highest educational attainment for thresholds with true positive rates > 0.6. Therefore, the high-school subpopulation would be erroneously flagged by an automatic decision-making system more often than other people with higher degrees.
        </p>
      </div>
    </div>
  </section>



  <section class="page-section" id="discussion">
    <div class="container">
      <div class="row justify-content-center">
        <h2 class="text-black mt-0">Discussion</h2>
        <div class="col-lg-8">
          <hr class="divider dark my-4">
          <p class="text-black-70 mb-4" style="line-height:2.3em;">
              Overall, there were no major differences in terms of model performance (AUC) between the methods that used sensitive attributes and those that did not. This disproves the common narrative of there being a trade-off between accuracy and privacy/bias.
<br>
<br>
              Both the risk of legal exposure for failing to protect this sensitive data and the long-term engineering benefits of keeping ML pipelines simple lead us to believe that it would be rational to minimize data collection. Age, education, gender and marital status are all quasi-identifiers, which in combination with other data might allow for the re-identification of the subjects in the dataset, and consequently the revealing of sensitive financial data associated with them. Including the variables in the dataset could also lead to human bias (even if it is implicit) for decision makers who would have access to the prediction, as well as to the rest of the profile of the subject. In our view, the only ethically justifiable reason for collecting these sensitive attributes would be for purposes of investigating algorithmic bias. However, our literature review has shown this not to be the case in practice.
              <br>
              <br>
              Concerning model selection, we would advise the use of logistic regression due to its interpretable characteristics and quick optimization procedure that can be harnessed without sacrificing much predictive power.
              <br>
              <br>
              In our disparate impact analysis, we find no evidence of bias against subpopulations that share a certain age, gender or marital status, which are protected categories in the US. In terms of level of education, we find a difference in performance for people with high school degrees. This might be problematic due to the fact that lower educational attainment is known to correlate with lower income, and thus we might be denying people who are at low risk of defaulting the credit they might be needing more than higher income populations. However, bias need not be the reason for the inferior performance. Maybe the economic trajectories of people whose highest educational degree is high school are more random / have higher variability than those of people with higher educational degrees, making them harder to predict. More research and inquiry into the dataset is needed to address such questions.
              <br>
              <br>
              Additionally, it is worth asking to what extent a US dataset would differ from the Taiwan one: would the same variables be the most important ones, would using sensitive attributes such as race make a bigger difference in accuracy, etc. Further research could focus on possible comparisons between models trained on different datasets.

          </p>
        </div>
      </div>
    </div>
  </section>

  <section class="page-section bg-primary" id="next">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-8 text-align-left">
          <p class="mb-4" style="line-height:2.3em;">
            Further possible avenues of research could be the following:
          </p>
          <h5 class="text-align-center">1. Feedback Loop Simulation</h5>
          <p class="mb-4" style="line-height:2.3em;">
            As mentioned above, a widely discussed issue in credit score assessment is the presence of feedback loops. If you are an individual who is initially denied for a loan or credit card line, then you might face additional financial hardships, such as not being able to purchase a home or car, or pay for unexpected costs. This might, in turn, negatively impact your credit score, making it even less likely for you to get further loans. This kind of feedback loops can also be seen in predictive policing, as described in a 2018 paper by Ensign et al. [10]. Employing a similar methodology, we would like to create simulations to prove the existence of such feedback loops, as well as possible interventions.
          </p>
          <h5 class="text-align-center">2. Categorizing practicioner's errors</h5>
          <p class="mb-4" style="line-height:2.3em;">
            As previously mentioned, many Kaggle users and papers have done work with this dataset. While investigating issues with unexplained categories (that we managed to get explanations for directly from the author of the original paper), we noticed several types of errors that were being made in the analysis, either in terms of the technical aspects of the models employed, or in the interpretation of the results. We aim to look at a larger number of these papers/notebook and create a classification for the most common type of errors, and discuss their implications.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section class="page-section" id="references">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-8 text-align-left">
          <h5 class="text-black mt-0">References</h2>
          <p class="mb-4" style="line-height:4em;">
            <ul style="list-style-type:none;line-height:2.2em;">
              <li>Find our Github repository <a href="https://github.com/JanGeffert/AC221-FinalProject">here</a>.</li>
              <li><br></li>
              <li>[1] Federal Trade Commission. (n.d.) Your Equal Credit Opportunity Rights. Retrieved from https://www.consumer.ftc.gov/articles/0347-your-equal-credit-opportunity-rights.
              </li><li>[2] Hurley, M., Adebayo, J. (2017). Credit Scoring in the Era of Big Data. Yale Journal of Law and Technology 18(1).
              </li><li>[3] Mayer, C. (November 10, 2017). How Does Gender Affect Credit Scores? Fiscal Tiger. Retrieved from https://www.fiscaltiger.com/gender-affect-credit-scores/.
              </li><li>[4] Yeh, I-C., Lien, C. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications 36(2): 2473-2480.
              </li><li>[5] Ortiz, L.M. (July 25, 2016). Challenging the Almighty Credit Score. Shelterforce. Retrieved from https://shelterforce.org/2016/07/25/challenging-the-almighty-credit-score/.
              </li><li>[6] Pritchard, J. (March 11, 2018). What is Credit? The Balance. Retrieved from https://www.thebalance.com/what-is-credit-315391.
              </li><li>[7] Rice, L., Swesnik, D. (June, 2012). Discriminatory Effects of Credit Scoring on Communities of Color. National Fair Housing Alliance. Retrieved from https://nationalfairhousing.org/wp-content/uploads/2017/04/NFHA-credit-scoring-paper-for-Suffolk-NCLC-symposium-submitted-to-Suffolk-Law.pdf
              </li><li>[8] Avery, R.B., Brevoort, K.P., Canner, G.B. Does Credit Scoring Produce a Disparate Impact? Federal Reserve. Retrieved from https://www.federalreserve.gov/pubs/feds/2010/201058/201058pap.pdf.
              </li><li>[9] Keyes, Os. The Misgendering Machines: Trans/HCI Implications of Automatic Gender Recognition. Retrieved from https://ironholds.org/resources/papers/agr_paper.pdf.
              </li><li>[10] Ensign, D., Friedler, S.A., Neville, S., Scheidegger, C., Venkatasubramanian, S. (2018). Runaway Feedback Loops in Predictive Policing. Proceedings of Machine LEarning Research 81: 1-12.
              </li>
            </ul>
          </p>
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="bg-light py-5">
    <div class="container">
      <div class="small text-center text-muted">Miruna G. Cristus, Everett Sussman, &amp; Jan Geffert, Spring 2019, cover image by <a href="https://unsplash.com/photos/na8l3EPqpvY">twopaddles</a>.</div>
    </div>
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
  <script src="vendor/magnific-popup/jquery.magnific-popup.min.js"></script>

  <!-- Custom scripts for this template -->
  <script src="js/creative.min.js"></script>

</body>

</html>
